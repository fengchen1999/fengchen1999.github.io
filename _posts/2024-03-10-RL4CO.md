---
layout: post
title: "Machine Learning for Combination Optimization"
categories: tech
tags: [reinforcement learning, discrete optimization]
---



According to the course of reinforcement learning (RL) taught by Prof. Dimitri P. Bertsekas, we try to take some notes to further understand how to use RL to solve decision problems, especially the combination optimization problem and discrete problem.



## Reinforcement Learning Basis

System
$$
\begin{align}
\color{red}{x_{k+1}=f_k(x_k,u_k), ~~ k=0,1,\dots,N-1,}
\end{align}
$$
where $x_k$: State, $u_k$: Control chosen from some set $U_k(x_k)$.

Given initial state $x_0$, minimize cost function $\eqref{obj:22}$ over control sequences $\{u_0,\dots,u_{N-1}\}$
$$
J(x_0;u_0,\dots,u_{N-1})=g_N(x_N)+\sum_{k=0}^{N-1}g_k(x_k,u_k)
\label{obj:22}
$$
whose optimal cost function is
$$
J^*(x_0)=\min_{\begin{aligned}u_k\in U_k(x_k), \\ k=0,\dots,N-1\end{aligned}} J(x_0;u_0,\dots,u_{N-1})
$$

#### Dynamic Programming Algorithm for Deterministic Problems

**Go backward** to compute the optimal costs $J_k^*(x_k)$ of the $x_k$-tail subproblems (<font color="red">off-line training</font> - involves lots of computation)  

 Start with 
$$
J_N^*(x_N)=g_N(x_N), \quad \forall x_N,
$$
and for $k=N-1,\dots,0$, let
$$
\color{red}{J_k^* = \min_{u_k\in U_k(x_k)}\left[g_k(x_k, u_k) + J_{k+1}^*(f_k(x_k, u_k))\right], \quad \forall x_k.}
$$
Then optimal cost $J^*(x_0)$ is obtained at the last step: $J_0^*(x_0)=J^*(x_0)$

**Go forward** to construct optimal control sequence $\{u_0^*,\dots,u_{N-1}^*\}$ (<font color="red">on-line play</font>)

Start with
$$
\color{red}{
u_0^*\in \arg \min_{u_0\in U_0(u_0)} \left[g_0(x_0, u_0) + J_1^*(f_0(x_0, u_0))\right], \quad x_1^*=f_0(x_0, u_0^*)
}
$$
Sequentially, going forward, for $k=1,2,\dots,N-1$, set
$$
\color{red}{
u_k^*\in \arg \min_{u_k\in U_k(x_k^*)} \left[g_k(x_k^*, u_0) + J_{k+1}^*(f_k(x_k^*, u_0))\right], \quad x_{k+1}^*=f_k(x_k^*, u_k^*)
}
$$

#### Approximation in Policy Space

We replace $J_k^*$ with an approximation $\tilde{J}_k$ during on-line play.
$$
\color{red}{
\tilde{u}_0 \in \arg \min_{u_0\in U_0(u_0)} \left[g_0(x_0, u_0) + \tilde{J}_1(f_0(x_0, u_0))\right], \quad \tilde{x}_1=f_0(x_0, \tilde{u}_0)
}
$$
Sequentially, going forward, for $k=1,2,\dots,N-1$, set
$$
\color{red}{
\tilde{u}_k\in \arg \min_{u_k\in U_k(\tilde{x}_k)} \left[g_k(\tilde{x}_k, u_0) + \tilde{J}_{k+1}(f_k(\tilde{x}_k, u_0))\right], \quad \tilde{x}_{k+1}=f_k(\tilde{x}_k, \tilde{u}_k)
}
$$
How to compute $\tilde{J}_{k+1}(x_{k+1})$? This is one of the **principal issues** in RL.

- <font color="red">Off-line problem approximation</font>: Use as $\tilde{J}_{k+1}$ the optimal cost function of a simpler problem, computed off-line by exact DP.
- <font color="red">Parametric cost approximation</font>: Obatin $\tilde{J}_{k+1}(x_{k+1})$ from a parametric class of functions $J(x_{k+1}, r)$, where $r$ is a parameter, e.g., training using data and a NN.
- <font color="red">Rollout with a heuristic</font>

**Note**: 1) the first method approximate the problem; 2) the second method approximate the problem and the search policy; 3) the third method approximate the search policy (seems safer).



#### Stochastic DP problem - Perfect State Observation (We Know $X_k$)

System $x_{k+1}=f_k(x_k, u_k, w_k)$ with random disturbance $w_k$ and Random Cost $g_k(x_k, u_k, w_k)$.

Cost function:
$$
E\left\{ g_N(x_N) + \sum_{k=0}^{N-1}g_k(x_k, u_k, w_k) \right\}
$$
<font color="red">Policies $\pi=\{\mu_0, \dots, \mu_{N-1}\}$, where $\mu_k$ is a "closed-loop control law" or "feedback policy" or a function of $x_k$. A "lookup table" for the control $u_k=\mu_k(x_k)$ to apply at $x_k$.</font> (The control sequence is not predetermined at $x_0$, we need to **Wait and See**)

Given initial state $x_0$, minimize cost function $\eqref{obj:2}$ over control sequences $\{\mu_0,\dots,\mu_{N-1}\}$
$$
\color{red}{
J_{\pi}(x_0;u_0,\dots,u_{N-1}) = E \left\{ g_N(x_N)+\sum_{k=0}^{N-1}g_k(x_k,\mu(x_k), w_k)\right\}
\label{obj:2}
}
$$
whose optimal cost function is <font color="red">$J^*(x_0)=\min_{\pi} J_{\pi}(x_0)$</font> and optimal policy is <font color="red">$J_{\pi^*}(x_0)=J^*(x_0)$</font>$\textcolor{red}{}$.
$$
J^*(x_0)=\min_{\pi} J(x_0;u_0,\dots,u_{N-1})
$$


#### Linear Quadratic Problem is a starting point for:

- <font color="red">Problem with safety/state constraints</font>
- <font color="red">Problem with control constraints</font>
- <font color="red">Unknown or changing system parameter</font>



![Approximations.png](https://s2.loli.net/2024/03/10/RBOCvYDESidATu7.png)

















#### Finite Horizon Deterministic Problem - Approximation in Value Space



#### Approximation in Policy Space



#### Infinite Horizon
