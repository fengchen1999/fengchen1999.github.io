---
layout: post
title: "Machine Learning for Combination Optimization"
categories: tech
tags: [reinforcement learning, discrete optimization]
---



According to the course of reinforcement learning (RL) taught by Prof. Dimitri P. Bertsekas, we try to take some notes to further understand how to use RL to solve decision problems, especially the combination optimization problem and discrete problem.



## Reinforcement Learning Basis

System
$$
x_{k+1}=f_k(x_k,u_k), \quad k=0,1,\dots,N-1,
$$
where $x_k$: State, $u_k$: Control chosen from some set $U_k(x_k)$.

Given initial state $x_0$, minimize cost function $\eqref{costFun}$ over control sequences $\{u_0,\dots,u_{N-1}\}$
$$
J(x_0;u_0,\dots,u_{N-1})=g_N(x_N)+\sum_{k=0}^{N-1}g_k(x_k,u_k)
\label{costFun}
$$
whose optimal cost function is
$$
J^*(x_0)=\min_{\begin{aligned}u_k\in U_k(x_k), \\ k=0,\dots,N-1\end{aligned}} J(x_0;u_0,\dots,u_{N-1})
$$

#### Dynamic Programming Algorithm for Deterministic Problems

**Go backward** to compute the optimal costs $J_k^*(x_k)$ of the $x_k$-tail subproblems (<font color="red">off-line training</font> - involves lots of computation)  

 Start with
$$
\color{red}{J_N^*(x_N)=g_N(x_N), \quad \forall x_N,}
$$
and for $k=N-1,\dots,0$, let
$$
\color{red}{J_k^* = \min_{u_k\in U_k(x_k)}\left[g_k(x_k, u_k) + J_{k+1}^*(f_k(x_k, u_k))\right], \quad \forall x_k.}
$$
Then optimal cost $J^*(x_0)$ is obtained at the last step: $\color{red}{J_0^*(x_0)=J^*(x_0)}$

**Go forward** to construct optimal control sequence $\{u_0^*,\dots,u_{N-1}^*\}$ ($\textcolor{red}{\text{on-line play}}$)

Start with
$$
\color{red}{
u_0^*\in \arg \min_{u_0\in U_0(u_0)} \left[g_0(x_0, u_0) + J_1^*(f_0(x_0, u_0))\right], \quad x_1^*=f_0(x_0, u_0^*)
}
$$
Sequentially, going forward, for $k=1,2,\dots,N-1$, set
$$
\color{red}{
u_k^*\in \arg \min_{u_k\in U_k(x_k^*)} \left[g_k(x_k^*, u_0) + J_{k+1}^*(f_k(x_k^*, u_0))\right], \quad x_{k+1}^*=f_k(x_k^*, u_k^*)
}
$$

#### Approximation in Policy Space

We replace $J_k^*$ with an approximation $\tilde{J}_k$ during on-line play.
$$
\color{red}{
\tilde{u}_0 \in \arg \min_{u_0\in U_0(u_0)} \left[g_0(x_0, u_0) + \tilde{J}_1(f_0(x_0, u_0))\right], \quad \tilde{x}_1=f_0(x_0, \tilde{u}_0)
}
$$
Sequentially, going forward, for $k=1,2,\dots,N-1$, set
$$
\color{red}{
\tilde{u}_k\in \arg \min_{u_k\in U_k(\tilde{x}_k)} \left[g_k(\tilde{x}_k, u_0) + \tilde{J}_{k+1}(f_k(\tilde{x}_k, u_0))\right], \quad \tilde{x}_{k+1}=f_k(\tilde{x}_k, \tilde{u}_k)
}
$$
How to compute $\tilde{J}_{k+1}(x_{k+1})$? This is one of the **principal issues** in RL.

- <font color="red">Off-line problem approximation</font>: Use as $\tilde{J}_{k+1}$ the optimal cost function of a simpler problem, computed off-line by exact DP.
- <font color="red">Parametric cost approximation</font>: Obatin $\tilde{J}_{k+1}(x_{k+1})$ from a parametric class of functions $J_(x_{k+1}, r)$, where $r$ is a parameter, e.g., training using data and a NN.
- <font color="red">Rollout with a heuristic</font>

**Note**: 1) the first method approximate the problem; 2) the second method approximate the problem and the search policy; 3) the third method approximate the search policy (seems safer).



#### Stochastic DP problem - Perfect State Observation (We Know $X_k$)

System $x_{k+1}=f_k(x_k, u_k, w_k)$ with random disturbance $w_k$ and Random Cost $g_k(x_k, u_k, w_k)$.

Cost function:
$$
E\left\{ g_N(x_N) + \sum_{k=0}^{N-1}g_k(x_k, u_k, w_k) \right\}
$$
<font color="red">Policies $\pi=\{\mu_0, \dots, \mu_{N-1}\}$, where $\mu_k$ is a "closed-loop control law" or "feedback policy" or a function of $x_k$. A "lookup table" for the control $u_k=\mu_k(x_k)$ to apply at $x_k$.</font> (The control sequence is not predetermined at $x_0$, we need to **Wait and See**)

Given initial state $x_0$, minimize cost function $\eqref{costFunc_stoc}$ over control sequences $\{\mu_0,\dots,\mu_{N-1}\}$
$$
\color{red}{
J_{\pi}(x_0;u_0,\dots,u_{N-1}) = E \left\{ g_N(x_N)+\sum_{k=0}^{N-1}g_k(x_k,\mu(x_k), w_k)\right\}
}
\label{costFunc_stoc}
$$
whose optimal cost function is <font color="red">$J^*(x_0)=\min_{\pi} J_{\pi}(x_0)$</font> and optimal policy is <font color="red">$J_{\pi^*}(x_0)=J^*(x_0)$</font>$\textcolor{red}{}$.
$$
\color{red}{
J^*(x_0)=\min_{\pi} J(x_0;u_0,\dots,u_{N-1})
}
$$

#### Linear Quadratic Problem is a starting point for:

- <font color="red">Problem with safety/state constraints</font>
- <font color="red">Problem with control constraints</font>
- <font color="red">Unknown or changing system parameters</font>



![Approximations.png](https://s2.loli.net/2024/03/10/RBOCvYDESidATu7.png)

















#### Finite Horizon Deterministic Problem - Approximation in Value Space



#### Approximation in Policy Space



#### Infinite Horizon



## Benders分解初探

考虑一个混合整数线性规划问题（MILP），其中整数变量 $x$ 被视为复杂的决策变量，其中 $\mathbf{c} \in \mathbb{R}^n, \mathbf{h} \in \mathbb{R}^p, \mathbf{G} \in \mathbb{R}^{m \times n}, \mathbf{H} \in \mathbb{R}^{n \times p}, \mathbf{b} \in \mathbb{R}^m$。
$$
\begin{equation}
	\begin{aligned}
		z^{MIP}:= \min \mathbf{c^T x + h^T y} \\
	\end{aligned}
	\label{eq:1}
\end{equation}
$$

$$
\text{s.t. } \mathbf{Gx + Hy \geq b},
\label{eq:2}
$$

$$
\label{eq:3}
\mathbf{x} \in \mathbb{Z}^n_+, \mathbf{y} \in \mathbb{R}^p_+,
$$

其拉格朗日形式：
$$
\begin{align}
L: & =\sum_{j \in J} c_j\lambda_j - \pi \left( \sum_{j \in J} a_j\lambda_j - b \right) - \sum_{j \in J} \alpha_j \lambda_j \\
& = \sum_{j \in J} (c_j - \pi a_j) \lambda_j + \pi b - \sum_{j \in J} \alpha_j \lambda_j
\end{align}
$$
我们可以定义 $\overline{c}_j = c_j - \pi a_j$ ，来描述 $\lambda_j$ 对于问题的投资收益。有两种理解：1）$\overline{c}_j$ 越小，$\lambda_j$ 投资收益越高，即在最优解的情况下，每增加投资1个单位 $\lambda_j$ ，目标函数增加 $c_j-\pi a_j$ ，其中 $\pi a_j$ 表示边际成本（或影子价格）。变量的影子价格是指该变量每增加1个单位所导致的目标函数的变化值（符号视系数而定）。2）$c_j$ 越小，目标函数越小；$\pi a_j$ 越大，越容易满足约束 $\eqref{eq:2}$ ，则 $\lambda_j$ 对于该问题是一个好的投资。

当 $|J|$ 很大时，计算成本过高，此时考虑列生产方法。令子集 $J^{'} \subseteq J$ ，此时称 $MP$ 为限制性主问题（$RMP$），对其求解得到 $\rm{\lambda}$ 和 $\rm{\pi}$ ，并且获得 $MP$ 的一个上界 $\overline{z}$ 。定义 $a_j,~~j \in J$ 构成集合 $\mathcal{A}$，求解子问题：
$$
\label{eq:4}
\overline{c}^*:= \min \{ c_j - \pi a_j ~~ | ~~ a_j \in \mathcal{A} \}
$$
如果 $\overline{c}^* \geq 0$ ，则 $RMP$ 的解即为 $MP$ 的解，求解完成。否则，将 $\overline{c}^*$ 所代表 $j$ 加入 $J^{'}$，即 $J^{'} = j \bigcup J^{'}$ ，进行下一轮的求解。这是一种启发式手段，即将投资收益最高的入基变量加入 $RMP$ 中。我们可以通过人工给定，启发式，预求解（"热启动"）的方式对问题进行初始化，即确定 $J^{'}$。

对于有限集合 $\mathcal{A}$ ，问题一定是收敛的，但是如何知道迭代过程中的解的质量（即收敛情况）呢。我们可以人工给定一个条件 $\kappa \geq \sum_{j \in J} \lambda_j$ ，则有：
$$
\overline{z} + \kappa \overline{c}^* \leq z_{MP}^* \leq \overline{z}
$$
这相当于在此时将所有投资押宝在了 $\lambda_j$ 上，实际上将资源不断注入 $\lambda_j$ 的过程中，影子价格 $\pi$ 会时时变化，即原来投资收益最高的 $j$ 可能会变化，或者条件 $\eqref{eq:2}$会遭到破坏，所以 $\overline{z} + \kappa \overline{c}^*$ 只是给出了 $MP$ 的下限。



### 总结

列生成的主要思想是：**通过去掉一些难的约束，求解一个相对容易的限制性主问题（$RMP$），得到主问题（$MP$）的下界（因为扩大了可行域）。再通过构造某种形式的子问题，把一些重要（紧）的约束加回到 $RMP$ 中，提高下界。不断迭代，直至收敛。**

注1：在该例子中，选取 $J^{'} \subseteq J$ ，等价于把一些变量设成了0，实际上是缩小了可行域，所以得到的是上界。迭代过程中慢慢地扩张可行域，压低上界，思想是一致的。

注2：列生产一般会有一个上下界不断逼近收敛的过程，实际上上界（该例是下界）并不是必须的，只是为了把握迭代过程中解的质量，对于某些难求解的问题可以提前终止求解，得知求解的Gap。
